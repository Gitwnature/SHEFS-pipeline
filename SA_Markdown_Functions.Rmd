---
title: "SA SHEFS pipeline"
output: html_notebook
---


Load packages to run Fiona's SA pipeline

```{r}

library(CoordinateCleaner)
library(dplyr)
library(here)
library(lwgeom)
library(raster)
library(readr)
#library(sdmpl)
library(dismo)
library(sf)
library(caret)
library(profvis)
library(randomForest)
library(taxize)
library(vroom)

```

**SKIP**

Stack BioClim layers in raster:
(Sophie: doesn't work for me, skip and access files in RDS joint project folder. Use workaround by Vivienne)
```{r}

loadBioclim <-  function(path, extension, extent = NULL, prjctn = NULL) {
    
    bio_layers <- list.files(path, pattern = paste0(extension, "$"), full.names = TRUE)
    bioclim <- raster::stack(bio_layers)
    
    bioclim <- raster::stack()    #don't think this section is needed, have replaced with the line above but maybe I'm missing something
    for (i in bio_layers) {
    bioclim <- raster::stack(bioclim, raster[i])
      }
    
    if (!is.null(extent)) {
      bioclim <- raster::crop(bioclim, extent)
    }
    
    if (!is.null(prjctn)) {
      if (class(prjctn) != "CRS") {
        stop("Projection must be an object of class CRS.")
      }
      bioclim <- raster::projectRaster(bioclim, crs = prjctn)
    }
    return(bioclim)
  }


```



Download records if there are more than 10 and fewer than 200,000; removie NAs and duplicates; keep records since 2010; keep "human observation," "living specimen," and "machine observation."
```{r}

gbifData <- function(sp_name, ext_sp = NULL, ext_occ , out_dir, min_occ = 0) {
  
  # Include something for if there is nothing in GBIF...
  gen <- strsplit(sp_name, "_")[[1]][1]
  sp <- strsplit(sp_name, "_")[[1]][2]
  
  if (is.null(ext_sp)){ext_sp = ext_occ}
  # count records.
  .count <- dismo::gbif(
    genus = gen,
    species = sp,
    #ext = ext_sp,
    geo = TRUE,
    removeZeros = TRUE,
    download = FALSE)
  
  # the minimum count of occurrences is 10
  if (.count >= 10 & .count <= 200000) {
    .xx <- dismo::gbif(
      genus = gen,
      species = sp,
      ext = ext_occ,
      geo = TRUE,
      removeZeros = TRUE,
      download = TRUE)
    
    # remove NAs and duplicates
    if (is.null(.xx)) {
      output_data <- NULL
    } else {
      if (all(c("lon", "lat") %in% colnames(.xx))) {
        .xx <- .xx %>% 
        dplyr::filter((basisOfRecord == "HUMAN_OBSERVATION" | basisOfRecord == "LIVING_SPECIMEN" | basisOfRecord == "MACHINE_OBSERVATION") & year >= 2010 )
        xx <- cbind(.xx$lon, .xx$lat)
        output_data <- matrix(unique(xx[complete.cases(xx), ]), ncol = 2)
        output_data <- cbind(sp_name, output_data)
        colnames(output_data) <- c("species", "x", "y")
        if(nrow(output_data) >= min_occ){
          write.csv(output_data, paste0(out_dir, "/", sp_name, ".csv"), row.names = FALSE)
        }
      } else { output_data <- NULL }
    }
  } else {output_data <- paste0(sp_name, " too many records") }
  
  print(paste(sp_name, "done!"))
  #return(output_data)
}

```


Apply Coordinate Cleaner package to database
```{r}

cc_wrapper <- function(sp_name, in_dir, out_dir, min_occ = 0){
  
  sp_df <- read.csv(paste0(in_dir,"/", sp_name, ".csv"))
  
  sp_cc <- CoordinateCleaner::clean_coordinates(sp_df, lon = "x", lat = "y", species = "species",tests = c("capitals","centroids","equal", "gbif","institutions","seas","zeros"))
  
  sp_df<-sp_df[which(sp_cc$.summary == TRUE),]

  
  print(paste(sp_name, "cleaned!"))
  
  if (nrow(sp_df)>= min_occ){
    write.csv(sp_df, paste0(out_dir, "/", sp_name, ".csv"), row.names = FALSE)
  }
}

```


Rarefy points to keep maximum 1 occurence per cell:
```{r}

rarefyPoints <- function(sp_name, in_dir, out_dir, ref_map, min_occ = 0 ){
  
  df <- read.csv(paste0(in_dir, "/", sp_name, ".csv"))
  pnts <- SpatialPointsDataFrame(matrix(c(df$x, df$y), ncol = 2), df)
  cells <- raster::cellFromXY(ref_map, pnts)
  pres_cells <- ref_map
  pres_cells[unique(cells)] <- 1
  
  rarefied_presence <- raster::rasterToPoints(pres_cells, fun = function(x) {x == 1})[, c(1, 2)]
  
  rarefied_presence <- sp::SpatialPoints(rarefied_presence)
  sp::proj4string(rarefied_presence) <- sp::proj4string(ref_map)
  df_rar <- data.frame(sp_name,rarefied_presence)
  
  print(paste0(sp_name, " rarefied!"))
  
  if (nrow(df_rar) >= min_occ){
    write.csv(df_rar, paste0(out_dir, "/", sp_name, ".csv"), row.names = FALSE)
  }
}

```


Make a SpatialPointsDataFrame; vroom function to read delimited file into Tibble; extract coordinates from the raster:

```{r}

ras_extract <- function(sp_name, in_dir, out_dir, raster_in) {
  
  df <- vroom::vroom(paste0(in_dir, "/", sp_name, ".csv"), delim = ",")
  xy <- sp::SpatialPointsDataFrame(matrix(c(df$x, df$y), ncol = 2), df)
  ras_ext <- raster::extract(raster_in, xy)
  pres_ext <- data.frame(df, ras_ext)
  pres_ext <- pres_ext[complete.cases(pres_ext),]
  write.csv(x = pres_ext,
            file = paste0(out_dir, "/", sp_name, ".csv"),
            row.names = FALSE)
  
  print(sp_name)
}

```

**SKIP**
Doesn't work for me, skip and access WWF shapefiles in RDS joint project folder: 
```{r WWF}

wwf_ecoregions_get <- function(){
  if(!dir.exists("WWF_Ecoregions")){
    dir.create("WWF_Ecoregions")
  }
  download.file("https://c402277.ssl.cf1.rackcdn.com/publications/15/files/original/official_teow.zip?1349272619", destfile = "WWF_Ecoregions.zip")
  unzip("WWF_Ecoregions.zip", exdir = "WWF_Ecoregions/")
  lf <- list.files("WWF_Ecoregions/", recursive = TRUE, full.names = TRUE)
  nlf <- basename(lf)
  file.rename(lf, paste0("WWF_Ecoregions/",nlf))
  file.remove("WWF_Ecoregions/official/")
}
```



Produce pseudo-absence data for each species:
```{r}

background_sampler <- function(sp_name, in_dir, out_dir, dens_abs = "absolute", density = NULL, no_pnts = NULL, type = "background", buffer = NULL, polygon = NULL){
  
  in_file <- list.files(in_dir, full.names = TRUE)[grepl(sp_name, list.files(in_dir))]
  
  sf_int <- read_csv(paste0(in_dir, "/", sp_name, ".csv")) %>%
    dplyr::select("x", "y") %>%
    dplyr::distinct() %>%
    # convert object into shapefile
    sf::st_as_sf(., coords = c("x", "y"), crs = 4326) %>% # coordinate reference system
    sf::st_intersection(., polygon)
  bkg_polygon <- polygon %>%
    dplyr::filter(ECO_NAME %in% sf_int$ECO_NAME)
  
  if (dens_abs == "density"){
    no_pnts <- round(as.numeric(sum(st_area(bkg_polygon)))/(1000000*density))   
  }
  
  if (type == "background"){
    points_out <- bkg_polygon %>% 
      sf::st_sample(., size = no_pnts, type = "random")  
  }
  
  if (type == "pseudoabsence"){
    diss_bkg_polygon <- sf::st_union(bkg_polygon)
    sf_int_trans <- st_transform(sf_int, "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m no_defs") #robinson projection instead of 54030
    buff_pnts <- sf::st_buffer(sf_int_trans, buffer*1000)  
    buff_pnts <- st_transform(buff_pnts, crs(sf_int)) #should maybe get the original crs and have that here instead of 4326
    buff_pnts <- sf::st_union(buff_pnts)
    diff_bkg_polygon <- sf::st_difference(diss_bkg_polygon, buff_pnts)  
    
    points_out <- diff_bkg_polygon %>% 
      sf::st_sample(., size = no_pnts, type = "random")
  }
 
  tibb <- as_tibble(points_out)
  sep_df <- tibb %>%
    mutate(x = unlist(purrr::map(tibb$geometry, 1)),
           y = unlist(purrr::map(tibb$geometry, 2))) %>%
    dplyr::select(x, y)
  df_out <- data.frame(sp_name, sep_df)
  write.csv(df_out, file = paste0(out_dir, "/", sp_name, ".csv"), row.names = FALSE)
  
  print(basename(sp_name))
  #return(df_out)
}

```

Called in Step 8
Evaluate presence and absence points using dismo::evaluate; model is bioclim & domain (the envelope model)
```{r}

clustEvalPa <- function(num, pres_pts, backg_pts, kfolds_p, kfolds_a, curr, mod) {
    pres_train <- pres_pts[kfolds_p != num,]
    pres_test <- pres_pts[kfolds_p == num,]
    backg_test <- backg_pts[kfolds_a == num,]
    if (mod == "bioclim") {.m <- dismo::bioclim(curr, pres_train)
    } else if (mod == "domain") {.m <- dismo::domain(curr, pres_train)}
    e <- dismo::evaluate(pres_test, backg_test, .m, curr)
    return(e)
  }

```

Called in Step 8
Tss stands for True Skill Statistic: evaluate predictive accuracy for an SDM. 

```{r tssCalc}

tssCalc <- function(eval) {
  
  res <- data.frame(threshold = eval@t, tss = apply(eval@confusion, 1, function(x) {
                      cm <- t(matrix(rev(x), nrow = 2))
                      dimnames(cm) <- list(pred = c("0", "1"), obs = c("0", "1"))
                      class(cm) <- "table"
                      sens <- caret::sensitivity(cm)
                      spec <- caret::specificity(cm)
                      tss <- sens + spec - 1
                      return(tss)
                    }))
  
  thresh <- res$threshold[which.max(res$tss)]
  
  return(thresh)
}

```

Called in Step 8
Use tssCalc to calculate the threshold for presence / absence in all 3 models

```{r getThr}

# this looks like the place to modify threshold for presence...?
getThresholds <- function(aucs) {
  thresholds <- vector(mode = "list", length = 5)
  names(thresholds) <- c("spec_sens", "no_omission", "prevalence", "equal_sens_spec", "tss")
  thresholds[[1]] <- sapply(aucs, function(x)
    dismo::threshold(x, "spec_sens"))
  thresholds[[2]] <- sapply(aucs, function(x)
    dismo::threshold(x, "no_omission"))
  thresholds[[3]] <- sapply(aucs, function(x)
    dismo::threshold(x, "prevalence"))
  thresholds[[4]] <- sapply(aucs, function(x)
    dismo::threshold(x, "equal_sens_spec"))
  thresholds[[5]] <- sapply(aucs, function(x)
    tssCalc(x))
  
  return(thresholds)
}


```

Called in Step 8
Measure Bioclim model performance as AUC, and use to weight the model's contribution to the ensemble: 

```{r fitBC}

fitBC <- function(sp_name,
                  pres_dir,
                  backg_dir,
                  predictor_names,
                  predictors,
                  pred_out_dir,
                  eval_out_dir,
                  model_out_dir,
                  overwrite,
                  threads = 4,
                  eval = TRUE) {

        print(sp_name)
        
        # 1) Loading and preparing predictors
        predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
        CHELSA_predictor_names <- paste0("CHELSA_bio10_", predictor_names)

        curr <- raster::dropLayer(predictors, which(!names(predictors) %in% CHELSA_predictor_names))
        pres_pts <- as.matrix(data.table::fread(paste0(pres_dir, "/", sp_name, ".csv"), select = c("x", "y")), ncol = 2)
    
        # 2) Fitting the BioClim model
        if (nrow(pres_pts) < 10) {
          cat("Fewer than 10 data points - cannot fit model!")
        } else {
        backg_pts <- as.matrix(data.table::fread(paste0(backg_dir, "/", sp_name, ".csv"), select = c("x", "y")))
      
        cat("Fitting bioclim model...\n")
        bc <- dismo::bioclim(curr, pres_pts)
        cat("Done.\n")
        cat("...\n")

        # 3) Save the model object "bc" here for later application to future climate scenarios
        if (!dir.exists(model_out_dir)) {
          dir.create(model_out_dir)
        }
      
        cat("Saving current bioclim model...\n")
        saveRDS(bc, file = paste0(model_out_dir, "/", sp_name, "_bioclim_model.RDS"))
      
        # 4) Evaluating the BioClim model: how well does it predict our known dataset?
        cat("Evaluating bioclim model...\n")
        set.seed(123) # to make sure we get the same random draw every time
        kfolds_p <- dismo::kfold(pres_pts, 4)
        set.seed(123)
        kfolds_a <- dismo::kfold(backg_pts, 4)
      
        if (.Platform$OS.type == "unix") {
          cl <- parallel::makeForkCluster(threads)
        } else {
          cl <- parallel::makeCluster(threads)
        }
      
        parallel::clusterExport(cl, varlist = c("pres_pts", "backg_pts", "kfolds_p", "kfolds_a", "curr", "clustEvalPa"), envir = environment())
      
        aucs <- parallel::clusterApply(cl, 1:4, function(x) {
        
          clustEvalPa(x, pres_pts, backg_pts, kfolds_p, kfolds_a, curr, mod = "bioclim")
        })
      
        parallel::stopCluster(cl)
        cat("Done.\n")
        cat("...\n")
        
        # 5) Predict using the BioClim model
        thresholds <- getThresholds(aucs)
        cat("Predicting from bioclim model...\n")
        res <- dismo::predict(curr, bc)
        cat("Done.\n")
        cat("...\n")
        cat("Writing bioclim predictions...\n")
        out_file <- paste0(sp_name, "_bioclim.tif")
      
        if (!dir.exists(pred_out_dir)) {
          dir.create(pred_out_dir)
        }
      
        # 6) save predictions!
        raster::writeRaster(res, filename = paste(pred_out_dir, out_file, sep = "/"), format = "GTiff", overwrite = overwrite)
        gc()
        cat("Done.\n")
        cat("...\n")
      
        if (!dir.exists(eval_out_dir)) {
        dir.create(eval_out_dir, recursive = TRUE)
        }
      
        if (eval) {
        evals <- list(sp_name = sp_name, model = "bioclim", aucs = aucs, thresholds = thresholds)
        #evals <- data.frame(sp_name = sp_name, model = "bioclim", aucs = unlist(aucs), thresholds = unlist(thresholds))
        save(evals, file = paste0(eval_out_dir, "/", sp_name, "_bioclim_eval.RDA"))
        #save(evals, file = paste0(eval_out_dir, "/", sp_name, "_bioclim_eval.csv"))      
        }
    }
}

```

Called in Step 9
Evaluate SDM, Random Forest

```{r clustEval}

clustEvalSdm <- function(num, sdm_set, kfolds, model, mod) {

  train <- sdm_set[kfolds != num,]
  test_p <- sdm_set[kfolds == num & sdm_set[, "pb"] == 1,]
  test_a <- sdm_set[kfolds == num & sdm_set[, "pb"] == 0,]
  
  if (mod == "glm") {
    .m <- stats::glm(stats::formula(model),
                     data = train,
                     family = binomial(link = "logit"))
  } else if (mod == "rf") {
    .m <- suppressWarnings(randomForest::randomForest(model, data = train))
  }
  e <- dismo::evaluate(test_p, test_a, .m)
  e
}

```



**SKIP**

```{r CHELSA}

#Rewritten  by Henry F. (the link has changed)

chelsa_bioclim_get <- function(layer){
  
  if(!dir.exists("CHELSA")){
    dir.create("CHELSA")
  }
  if(!file.exists(paste0("CHELSA/CHELSA_bio10_",stringr::str_pad(layer, 2, pad = "0"), ".tif"))){
    download.file(paste0("ftp://envidatrepo.wsl.ch/uploads/chelsa/chelsa_V1/climatologies/bio/CHELSA_bio10_",stringr::str_pad(layer, 2, pad = "0"),".tif"), destfile = paste0("CHELSA/CHELSA_bio10_",stringr::str_pad(layer, 2, pad = "0"), ".tif"))
  } else{
    print("file already downloaded")
  }
}

```

**SKIP**

Function for counting the number of records for a given species:

```{r}

gbifCountData <- function(sp_name, ext_sp) {

  # Include something for if there is nothing in GBIF...
  gen <- strsplit(sp_name, "_")[[1]][1]
  sp <- strsplit(sp_name, "_")[[1]][2]
  
  # count records.
  .count <- dismo::gbif(
    # genus name
    genus = gen, 
    # species name (use '*' to download the entire genus; append '*' to the species name to get all naming variants and sub-taxa)
    species = sp, 
    # extent object to limite the geographic extent of the records. Create using functions like drawExtent and extent
    ext = ext_sp, 
    # a SpatialPointsDataFrame will be returned
    geo = TRUE, 
    # all records that have a lat OR a lon of zero will be removed. If FALSE, only records that have both a lat AND a lon of zero are deleted.
    removeZeros = TRUE, 
    # records not downloaded, but number of records are shown
    download = FALSE) 
  
  output_data <-  data.frame(sp_name, .count)
  print(paste(sp_name, "done!"))
  return(output_data)
}

```

Example: 

gbifCountData("gloriosa modesta")
drawExtent("gloriosa modesta")


Called in step 9
Measure GLM model performance as AUC, use to weight GLM contribution to the ensemble:

```{r fitGLM}

fitGLM <- function(sp_name,
                   pres_dir,
                   backg_dir,
                   predictor_names,
                   predictors,
                   pred_out_dir,
                   eval_out_dir,
                   model_out_dir,
                   overwrite, 
                   threads = 4, 
                   eval = TRUE) {
      print(sp_name)
      
      # 1) Prepping predictors from current data
      predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
      CHELSA_predictor_names <- paste0("CHELSA_bio10_", predictor_names)
    
      curr <-raster::dropLayer(predictors, which(!names(predictors) %in% CHELSA_predictor_names))
    
      model <- stats::formula(paste("pb ~", paste(CHELSA_predictor_names, collapse = "+")))
      pres_pts <- data.frame(pb = 1, data.table::fread(paste0(pres_dir, "/", sp_name, ".csv"), select = CHELSA_predictor_names))
    
    
      if (nrow(pres_pts) < 10) { cat("Fewer than 10 data points - cannot fit model!")
        } else {
        backg <- data.frame(pb = 0, data.table::fread(paste0(backg_dir, "/", sp_name, ".csv"), select = CHELSA_predictor_names))
        sdm_set <- rbind(pres_pts, backg)
      
      # 2) Fitting GLM
      
      cat("Fitting GLM...\n")
      glm <- stats::glm(stats::formula(model), data = sdm_set, family = binomial(link = "logit"))
      cat("Done.\n")
      cat("...\n")
      
      # 3) Save the model object "m" here for later application to future climate scenarios
      cat("Saving GLM model...\n")
      saveRDS(glm, file = paste0(model_out_dir, "/", sp_name, "_GLM.RDS"))
      cat("Done.\n")
      
      # 4) Evaluating GLM
      if (eval) {
        cat("Evaluating GLM...\n")
        set.seed(123)
        kfolds <- dismo::kfold(sdm_set, 4)
        
        if (.Platform$OS.type == "unix") {
          cl <- parallel::makeForkCluster(4)
        } else {
          cl <- parallel::makeCluster(4)
          parallel::clusterExport(cl, varlist = c("sdm_set", "kfolds", "model", "clustEvalSdm"), envir = environment())
          
          parallel::clusterCall(cl, function()
            library(dismo))
        }
        
        aucs <- parallel::clusterApply(cl, 1:4, function(x) {
          clustEvalSdm(x, sdm_set, kfolds, model, mod = "glm")
        })
        
        parallel::stopCluster(cl)
        cat("Done.\n")
        cat("...\n")
        thresholds <- getThresholds(aucs)
      }
      
      # 5) Predicting from GLM
      cat("Predicting from GLM...\n")
      res <- dismo::predict(curr, glm)
      res <- raster:::calc(res, fun = function(x) {
            exp(x) / (1 + exp(x))
          }
        ) #backtransforming from logit space
      
      cat("Done.\n")
      cat("...\n")
      cat("Writing GLM predictions...\n")
      out_file <- paste0(sp_name, "_glm.tif")
    
      if (!dir.exists(pred_out_dir)) {
        dir.create(pred_out_dir)
      }
      
      # 6) Save predictions!
      raster::writeRaster(res, filename = paste(pred_out_dir, out_file, sep = "/"), format = "GTiff", overwrite = overwrite)
      gc()
      cat("Done.\n")
      cat("...\n")
      
      if (!dir.exists(eval_out_dir)) {
        dir.create(eval_out_dir, recursive = TRUE)
      }
      
      if (eval) {
        evals <- list(sp_name = sp_name, model = "glm", aucs = aucs, thresholds = thresholds)
        save(evals, file = paste0(eval_out_dir, "/", sp_name, "_glm_eval.RDA"))
      }
    }
  }

```

Called in Step 9
Measure Random Forest model performance as AUC, use to weight RF contribution to the ensemble

```{r fitRF}

fitRF <- function(sp_name,
                  pres_dir,
                  backg_dir,
                  predictor_names,
                  predictors, 
                  pred_out_dir, 
                  eval_out_dir,  
                  model_out_dir,
                  overwrite,  
                  eval = TRUE) {
  
    print(sp_name)
  
    # 1) Prepping predictors from current data
    predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
    CHELSA_predictor_names <- paste0("CHELSA_bio10_", predictor_names)
    
    curr <- raster::dropLayer(predictors, which(!names(predictors) %in% CHELSA_predictor_names))
    model <- stats::formula(paste("pb ~", paste(CHELSA_predictor_names, collapse = "+")))
    pres <- data.frame(pb = 1, data.table::fread(paste0(pres_dir, "/", sp_name, ".csv"), select = CHELSA_predictor_names))
  
    if (nrow(pres) < 10) {
      cat("Fewer than 10 data points - cannot fit model!")
    } else {
    backg <- data.frame(pb = 0, data.table::fread(paste0(backg_dir, "/", sp_name, ".csv"), select = CHELSA_predictor_names))
    sdm_set <- rbind(pres, backg)
    sdm_set <- sdm_set[complete.cases(sdm_set),]  #should remove this line and just have no NAs background data
    
    # 2) Fitting the RF model
    cat("Fitting random forest model...\n")
    rf <- suppressWarnings(randomForest::randomForest(model, data = sdm_set))
    cat("Done.\n")
    cat("...\n")
    
    # 3) Save the model object "rf" here for later predictions with future climate scenarios
    cat("Saving random forest model...\n")
    saveRDS(rf, file = paste0(model_out_dir, "/", sp_name, "_randomforest_model.RDS"))
    cat("Done.\n")
    cat("...\n")
    
    # 4) Evaluating the RF model
    if (eval) {
      cat("Evaluating Random Forest model...\n")
      set.seed(123)
      kfolds <- dismo::kfold(sdm_set, 4)
      
      if (.Platform$OS.type == "unix") {
        cl <- parallel::makeForkCluster(4)
      } else {
        cl <- parallel::makeCluster(4)
      }
      parallel::clusterExport(cl, varlist = c("sdm_set", "kfolds", "model", "clustEvalSdm"), envir = environment()
      )
      
      aucs <- parallel::clusterApply(cl, 1:4, function(x) { clustEvalSdm(x, sdm_set, kfolds, model, mod = "rf")
      })
      
      parallel::stopCluster(cl)
      cat("Done.\n")
      cat("...\n")
      thresholds <- getThresholds(aucs)
    }
    
    # 5) Predicting with the RF model
    cat("Predicting from random forest...\n")
    
    res <- dismo::predict(curr, rf)
    cat("Done.\n")
    cat("...\n")
    cat("Writing random forest predictions...\n")
    out_file <- paste0(sp_name, "_rf.tif")
    
    if (!dir.exists(pred_out_dir)) {
      dir.create(pred_out_dir)
    }
    
    # Save predictions
    raster::writeRaster(res, filename = paste(pred_out_dir, out_file, sep = "/"), format = "GTiff", overwrite = overwrite)
    gc()
    cat("Done.\n")
    cat("...\n")
    
    if (!dir.exists(eval_out_dir)) {
      dir.create(eval_out_dir, recursive = TRUE)
    }
    
    if (eval) {
      evals <- list(sp_name = sp_name, model = "rf", aucs = aucs, thresholds = thresholds)
      save(evals, file = paste0(eval_out_dir, "/", sp_name, "_rf_eval.RDA"))
    }
  }
}

```

Called in step 11
Restructure files, extract AUC number from the model / spp into a dataframe

```{r get_eval}

# thresholds: 
#"spec_sens", "no_omission", "prevalence", "equal_sens_spec", "tss"

get_eval <- function(eval_file, 
                     threshold) {
  load(eval_file)
  model <- evals$model
  species <- evals$sp_name
  aucs <- mean(sapply(evals$aucs, function(x)
    x@auc))
  
  if (model == "glm") {
    thresholds  <- mean(exp(evals$thresholds[[threshold]]) / (1 + exp(evals$thresholds[[threshold]])))
  } else {
    thresholds  <- mean(evals$thresholds[[threshold]])
  }
  df_out <- data.frame(sp_name = species, model = model, auc = aucs, threshold = thresholds)
  
  return(df_out)
}

```


Called in step 12

Three methods: 
  - Majority PA method: take final ensemble democratically
  - Weighted: make mean of the 3 models to weigh
  - Mean: take straight mean of 3 (doesn't make sense)

```{r ensMod}

ensemble_model <- function(sp_name, eval_df, preds, out_dir, method = "weighted") {
    
    preds_f <- raster::stack(preds[grepl(sp_name, preds)])
    order <- gsub(paste0(sp_name, "_"), "" , names(preds_f))
    evals_f <- eval_df %>%
      dplyr::filter(sp_name == sp_name) 
    
    aucs <- eval_df$auc
    
    if (all(order == eval_df$model)) { 
      if (method == "majority_pa") {
        ens_preds <- preds_f > eval_df$threshold
        ens_pa <- sum(ens_preds)
        ens_pa[ens_pa < round(raster::nlayers(preds_f))] <- 0
        ens_pa[ens_pa >= round(raster::nlayers(preds_f))] <- 1
        ens_out <- ens_pa
      }
    
      if (method == "weighted") {
        preds_w <- preds_f * aucs
        preds_sum <- sum(preds_w)
        ens_out <- preds_sum / sum(aucs)
      }
      
      if (method == "mean") {
        ens_out <- raster::calc(preds_f, mean, na.rm = TRUE)
      }
    }
    
    gc()
    raster::writeRaster(ens_out, paste0(out_dir, "/",method, "/",sp_name, "_ensemble.tif"), overwrite = TRUE)
    return(ens_out)
  }

```

maj_pa_AN <- ensemble_model(
  sp_name = sp_name,
  eval_df = eval_df,
  preds = preds,
  method = "majority_pa",
  out_dir = here::here("predictions/ensemble")
)

Function called in step 14
Predict future species distribution based on previously saved GLM with future HadGEM climate model

fitBC_future(
  sp_name = sp_name,
  predictor_names = bioclim_layers,
  future_predictors = env_crop_4045,
  model_in_dir = here::here("models/"),
  scenario = 4045,
  pred_out_dir = here::here("predictions/bioclim/"),
  overwrite = TRUE,
)

```{r fitBCfuture}

fitBC_future <- function(sp_name, 
                         predictor_names, 
                         future_predictors, 
                         model_in_dir,
                         scenario,
                         pred_out_dir, 
                         overwrite,...) {

        print(sp_name)
        
        # 1) Load the future predictors
        predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
    
        CHELSA_predictor_names <- paste0("CHELSA_bio10_", predictor_names)

        ft <- raster::dropLayer(future_predictors, which(!names(future_predictors) %in% CHELSA_predictor_names))


        # 2) Load the BioClim model from model_in_dr
        cat("Loading bioclim model...\n")
        bc <- readRDS(file.path(model_in_dir, paste0(sp_name, "_bioclim_model.RDS")))
        cat("Done.\n")
        cat("...\n")  
    
        # 3) Predict using the BioClim model
        cat("Predicting from bioclim model...\n")
        res_f <- dismo::predict(ft, bc)
        cat("Done.\n")
        cat("...\n")
        
        if (!dir.exists(pred_out_dir)) {
          dir.create(pred_out_dir)
        }
        
        # 4) Save predictions
        cat("Writing bioclim predictions...\n")
        out_file <- paste0(sp_name, "_", scenario, "_bioclim.tif")
      
      
        raster::writeRaster(res_f, filename = paste(pred_out_dir, out_file, sep = "/"), format = "GTiff", overwrite = overwrite)
        gc()
        cat("Done.\n")
}

```


Function called in step 14
Predict future species distribution based on previously saved GLM with future HadGEM climate model

```{r fitGLMfuture}

fitGLM_future <- function(sp_name, 
                          predictor_names, 
                          future_predictors, 
                          model_in_dir,
                          pred_out_dir, 
                          scenario,
                          overwrite,...) {
  
      print(sp_name)
  
      predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
      predictor_names <- paste0("CHELSA_bio10_", predictor_names)
    
      # 1) Loading future bioclimatic predictors
      ft <- raster::dropLayer(future_predictors, which(!names(future_predictors) %in% predictor_names))
      
      # 2) Loading GLM from model_in_dir
      cat("Loading GLM model...\n")
      glm <- readRDS(file.path(model_in_dir, paste0(sp_name, "_GLM.RDS")))
      cat("Done.\n")
      cat("...\n")
    
      # 3) Predictions from GLM
      cat("Predicting from GLM...\n")
      res_f <- dismo::predict(ft, glm)
      res_f <- raster:::calc(res_f, fun = function(x) {
            exp(x) / (1 + exp(x))
          }
        )
      cat("Done.\n")
      cat("...\n")
      
      # 4) Save the predictions to pred_out_dir
      cat("Writing GLM predictions...\n")
      out_file_f <- paste0(sp_name, "_", scenario, "_glm.tif")
      
      if (!dir.exists(pred_out_dir)) {
        dir.create(pred_out_dir)
      }
      
      raster::writeRaster(res_f, filename = paste(pred_out_dir, out_file_f, sep = "/"), format = "GTiff", overwrite = overwrite)
      gc()
      cat("Done.\n")
  }

```

Called in step 14
Fitting RF to HadGEM future climate data

```{r fitRFfuture}

fitRF_future <- function(sp_name, 
                         pres_dir, 
                         backg_dir, 
                         predictor_names, 
                         future_predictors, 
                         model_in_dir,
                         pred_out_dir, 
                         scenario, 
                         overwrite) {
  
    print(sp_name)
    
    # 1) Load the future predictors
    CHELSA_predictor_names <- stringr::str_pad(predictor_names, 2, pad = "0")
    CHELSA_predictor_names <- paste0("CHELSA_bio10_", CHELSA_predictor_names)
    
    ft <- raster::dropLayer(future_predictors, which(!names(future_predictors) %in% CHELSA_predictor_names))
  
    # 2) Loading the RF model
    cat("Loading random forest model...\n")
    rf <- readRDS(file.path(model_in_dir, paste0(sp_name, "_randomforest_model.RDS")))
    cat("Done.\n")
    cat("...\n")
    
    # 3) Future predictions with the RF model
    cat("Predicting from random forest...\n")
    res_f <- dismo::predict(ft, rf)
    cat("Done.\n")
    cat("...\n")
    
    # 4) Save the predictions
    cat("Writing random forest predictions...\n")
    out_file <- paste0(sp_name, "_", scenario, "_rf.tif") # filename specific to future dates and rcp scenario
    
    if (!dir.exists(pred_out_dir)) {
      dir.create(pred_out_dir)
    }
    
    raster::writeRaster(res_f, filename = paste(pred_out_dir, out_file, sep = "/"), format = "GTiff", overwrite = overwrite)
    gc()
    cat("Done.\n")
  }

```

Called in step 15
Future version because the evals_df order includes the future scenario name and therefore must be updated to not fail the if(all(order == eval_df$model)) clause: 
```{r ensModfuture}

ensemble_model_future <- function(sp_name, 
                                  eval_df, 
                                  preds, 
                                  scenario,
                                  out_dir, 
                                  method = "weighted") {
    
    preds_f <- raster::stack(preds[grepl(sp_name, preds)])
    order <- gsub(paste0(sp_name, "_", scenario, "_"), "" , names(preds_f))
    evals_f <- eval_df %>%
      dplyr::filter(sp_name == sp_name) 
    
    aucs <- eval_df$auc
    
    
    if (all(order == eval_df$model)) { 
      if (method == "majority_pa") {
        ens_preds <- preds_f > eval_df$threshold
        ens_pa <- sum(ens_preds)
        ens_pa[ens_pa < round(raster::nlayers(preds_f))] <- 0
        ens_pa[ens_pa >= round(raster::nlayers(preds_f))] <- 1
        ens_out <- ens_pa
      }
    
      if (method == "weighted") {
        preds_w <- preds_f * aucs
        preds_sum <- sum(preds_w)
        ens_out <- preds_sum / sum(aucs)
      }
      
      if (method == "mean") {
        ens_out <- raster::calc(preds_f, mean, na.rm = TRUE)
      }
    }
    
    if (!dir.exists(out_dir)) {
          dir.create(out_dir)
        }
    
    gc()
    raster::writeRaster(ens_out, paste0(out_dir, "/",method, "/",sp_name, "_ensemble.tif"), overwrite = TRUE)
    return(ens_out)
  }
```

